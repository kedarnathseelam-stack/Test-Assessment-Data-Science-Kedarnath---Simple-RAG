# -*- coding: utf-8 -*-
"""llm

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZlvkMpgPEtwdbSzBLHgRp1pgty55iTRK
"""

from transformers import pipeline
from cache import load_cache, save_cache
from config import settings

generator = pipeline("text2text-generation", model=settings.LLM_MODEL)

def generate_answer(query, context):
    key = f"llm_{query}_{context[:50]}"
    cached = load_cache(key)
    if cached:
        return cached

    prompt = f"Answer based on context:\n\n{context}\n\nQuestion: {query}"
    output = generator(prompt, max_length=200, do_sample=False)[0]["generated_text"]

    save_cache(key, output)
    return output







